# -*- coding: utf-8 -*-
"""GAN with LSTM with ECG Data v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SqvblDLd003py6uGuXwGaFtqi1dphyni

#Introduction
This is the simple script to use a 2 layer LSTM with minibatch discrimination in the discriminator and a 2 -layer LSTM in the generator.
"""

import torch
from tqdm import tqdm
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import torch.nn as nn

# For testing using the MNIST dataset
from torchvision import transforms, datasets
from torch.autograd.variable import Variable
from torch.utils.data import Dataset, DataLoader
sns.set(rc={'figure.figsize':(11, 4)})

import datetime 
from datetime import date
today = date.today()

import random
import json as js
import pickle

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))
print(torch.cuda.memory_allocated()) # Shows the amount of memory used by the tensors
print(torch.cuda.memory_cached()) # shows the amount of memory cached

CUDA_LAUNCH_BLOCKING=1

"""#Define MMD

PDIST code comes from torch-two-sample utils code: https://github.com/josipd/torch-two-sample/blob/master/torch_two_sample/util.py
"""

def pdist(sample_1, sample_2, norm=2, eps=1e-5):
    r"""Compute the matrix of all squared pairwise distances.
    Arguments
    ---------
    sample_1 : torch.Tensor or Variable
        The first sample, should be of shape ``(n_1, d)``.
    sample_2 : torch.Tensor or Variable
        The second sample, should be of shape ``(n_2, d)``.
    norm : float
        The l_p norm to be used.
    Returns
    -------
    torch.Tensor or Variable
        Matrix of shape (n_1, n_2). The [i, j]-th entry is equal to
        ``|| sample_1[i, :] - sample_2[j, :] ||_p``."""
    n_1, n_2 = sample_1.size(0), sample_2.size(0)
    norm = float(norm)
    
    if norm == 2.:
        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)
        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)
        norms = (norms_1.expand(n_1, n_2) +
                 norms_2.transpose(0, 1).expand(n_1, n_2))
        distances_squared = norms - 2 * sample_1.mm(sample_2.t())
        return torch.sqrt(eps + torch.abs(distances_squared))
    else:
        dim = sample_1.size(1)
        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)
        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)
        differences = torch.abs(expanded_1 - expanded_2) ** norm
        inner = torch.sum(differences, dim=2, keepdim=False)
        return (eps + inner) ** (1. / norm)

"""Bringing in required dependencies as defined in the GitHub repo: https://github.com/josipd/torch-two-sample/blob/master/torch_two_sample/permutation_test.pyx"""

from __future__ import division

def permutation_test_mat(matrix,
                         n_1,  n_2,  n_permutations,
                          a00=1,  a11=1,  a01=0):
    """Compute the p-value of the following statistic (rejects when high)
        \sum_{i,j} a_{\pi(i), \pi(j)} matrix[i, j].
    """
    n = n_1 + n_2
    pi = np.zeros(n, dtype=np.int8)
    pi[n_1:] = 1

    larger = 0.
    count = 0
    
    for sample_n in range(1 + n_permutations):
        count = 0.
        for i in range(n):
            for j in range(i, n):
                mij = matrix[i, j] + matrix[j, i]
                if pi[i] == pi[j] == 0:
                    count += a00 * mij
                elif pi[i] == pi[j] == 1:
                    count += a11 * mij
                else:
                    count += a01 * mij
        if sample_n == 0:
            statistic = count
        elif statistic <= count:
            larger += 1

        np.random.shuffle(pi)

    return larger / n_permutations

"""Code from Torch-Two-Samples at https://torch-two-sample.readthedocs.io/en/latest/#"""

class MMDStatistic:
    r"""The *unbiased* MMD test of :cite:`gretton2012kernel`.

    The kernel used is equal to:

    .. math ::
        k(x, x') = \sum_{j=1}^k e^{-\alpha_j\|x - x'\|^2},

    for the :math:`\alpha_j` proved in :py:meth:`~.MMDStatistic.__call__`.

    Arguments
    ---------
    n_1: int
        The number of points in the first sample.
    n_2: int
        The number of points in the second sample."""

    def __init__(self, n_1, n_2):
        self.n_1 = n_1
        self.n_2 = n_2

        # The three constants used in the test.
        self.a00 = 1. / (n_1 * (n_1 - 1))
        self.a11 = 1. / (n_2 * (n_2 - 1))
        self.a01 = - 1. / (n_1 * n_2)

    def __call__(self, sample_1, sample_2, alphas, ret_matrix=False):
        r"""Evaluate the statistic.

        The kernel used is

        .. math::

            k(x, x') = \sum_{j=1}^k e^{-\alpha_j \|x - x'\|^2},

        for the provided ``alphas``.

        Arguments
        ---------
        sample_1: :class:`torch:torch.autograd.Variable`
            The first sample, of size ``(n_1, d)``.
        sample_2: variable of shape (n_2, d)
            The second sample, of size ``(n_2, d)``.
        alphas : list of :class:`float`
            The kernel parameters.
        ret_matrix: bool
            If set, the call with also return a second variable.

            This variable can be then used to compute a p-value using
            :py:meth:`~.MMDStatistic.pval`.

        Returns
        -------
        :class:`float`
            The test statistic.
        :class:`torch:torch.autograd.Variable`
            Returned only if ``ret_matrix`` was set to true."""
        sample_12 = torch.cat((sample_1, sample_2), 0)
        distances = pdist(sample_12, sample_12, norm=2)

        kernels = None
        for alpha in alphas:
            kernels_a = torch.exp(- alpha * distances ** 2)
            if kernels is None:
                kernels = kernels_a
            else:
                kernels = kernels + kernels_a

        k_1 = kernels[:self.n_1, :self.n_1]
        k_2 = kernels[self.n_1:, self.n_1:]
        k_12 = kernels[:self.n_1, self.n_1:]

        mmd = (2 * self.a01 * k_12.sum() +
               self.a00 * (k_1.sum() - torch.trace(k_1)) +
               self.a11 * (k_2.sum() - torch.trace(k_2)))
        if ret_matrix:
            return mmd, kernels
        else:
            return mmd


    def pval(self, distances, n_permutations=1000):
        r"""Compute a p-value using a permutation test.

        Arguments
        ---------
        matrix: :class:`torch:torch.autograd.Variable`
            The matrix computed using :py:meth:`~.MMDStatistic.__call__`.
        n_permutations: int
            The number of random draws from the permutation null.

        Returns
        -------
        float
            The estimated p-value."""
        if isinstance(distances, Variable):
            distances = distances.data
        return permutation_test_mat(distances.cpu().numpy(),
                                    self.n_1, self.n_2,
                                    n_permutations,
                                    a00=self.a00, a11=self.a11, a01=self.a01)

"""This paper https://arxiv.org/pdf/1611.04488.pdf says that the most common way to calculate sigma is to use the median pairwise distances between the joint data"""

def pairwisedistances(X,Y,norm=2):
    dist = pdist(X,Y,norm)
    return np.median(dist.numpy())

"""#Load Training Set"""

class ECGData(Dataset):
  #This is the class for teh ECG Data that we need to load, transform and then use in teh dataloader.
  def __init__(self,source_file,class_id, transform = None):
    self.source_file = source_file
    data = pd.read_csv(source_file, header = None)
    class_data = data[data[187]==class_id]
    self.data = class_data.drop(class_data.iloc[:,130:188],axis=1)
    self.transform = transform
    self.class_id = class_id
    
  def __len__(self):
    return self.data.shape[0]
    
  def __getitem__(self,idx):
    sample = self.data.iloc[idx]
    if self.transform:
        sample = self.transform(sample)
    return sample

class PD_to_Tensor(object):
    def __call__(self,sample):
      return torch.tensor(sample.values).cuda()

def GetECGData(source_file,class_id):
  compose = transforms.Compose(
        [PD_to_Tensor()
        ])
  return ECGData(source_file ,class_id = class_id, transform = compose)

#Taking normal ECG data for now
source_filename = './mitbih_train.csv'
data = GetECGData(source_file = source_filename,class_id = 0)

print(len(data))
print(data[3])

sample_size = 119 #batch size needed for Data Loader and the noise creator function.

# Create loader with data, so that we can iterate over it

data_loader = torch.utils.data.DataLoader(data, batch_size=sample_size, shuffle=True)
# Num batches
num_batches = len(data_loader)
print(num_batches)

"""#Get Test Data"""

test_filename =  './mitbih_test.csv'

data_test = GetECGData(source_file = test_filename,class_id = 0)

print(len(data_test))

data_loader_test = torch.utils.data.DataLoader(data_test[:18088], batch_size=sample_size, shuffle=True)

"""#Defining NN Modules

## LSTM Class
"""

class LSTM_class(torch.nn.Module):
  def __init__(self,seq_length,batch_size,n_features = 1, hidden_dim = 512, num_layers = 2 ):
      super(LSTM_class,self).__init__()
      self.n_features = n_features
      self.hidden_dim = hidden_dim
      self.num_layers = num_layers
      self.seq_length = seq_length
      self.batch_size = batch_size
      
      self.layer1 = torch.nn.LSTM(input_size = self.n_features, hidden_size = self.hidden_dim, num_layers = self.num_layers,batch_first = True,dropout = 0, )
      self.out = torch.nn.Sequential(torch.nn.Linear(self.hidden_dim,n_features),torch.nn.Sigmoid()) # to make sure the output is between 0 and 1
      
  def init_hidden(self):
      weight = next(self.parameters()).data
      hidden = (weight.new(self.num_layers, self.batch_size, self.hidden_dim).zero_().cuda(), weight.new(self.num_layers, self.batch_size, self.hidden_dim).zero_().cuda())
      return hidden
  
  def forward(self,x,hidden):
      x,hidden = self.layer1(x.view(self.batch_size,self.seq_length,1),hidden)
      x = self.out(x)
      return x,hidden

"""##Minibtach Discrimination
Creating a module for Minibatch Discrimination to avoid mode collapse as described:
https://arxiv.org/pdf/1606.03498.pdf
https://torchgan.readthedocs.io/en/latest/modules/layers.html#minibatch-discrimination
"""

# The aim of this module is to 

class MinibatchDiscrimination(torch.nn.Module):
   def __init__(self,input_features,output_features,minibatch_normal_init, hidden_features=16):
      super(MinibatchDiscrimination,self).__init__()
      
      self.input_features = input_features
      self.output_features = output_features
      self.hidden_features = hidden_features
      self.T = torch.nn.Parameter(torch.randn(self.input_features,self.output_features, self.hidden_features))
      if minibatch_normal_init == True:
        nn.init.normal(self.T, 0,1)
      
   def forward(self,x):
      M = torch.mm(x,self.T.view(self.input_features,-1))
      M = M.view(-1, self.output_features, self.hidden_features).unsqueeze(0)
      M_t = M.permute(1, 0, 2, 3)
      # Broadcasting reduces the matrix subtraction to the form desired in the paper
      out = torch.sum(torch.exp(-(torch.abs(M - M_t).sum(3))), dim=0) - 1
      
      return torch.cat([x, out], 1)

"""##Discriminator"""

class Discriminator(torch.nn.Module):
  def __init__(self,seq_length,batch_size,minibatch_normal_init,minibatch = 0,n_features = 1, hidden_dim = 50, num_layers = 2 ):
      super(Discriminator,self).__init__()
      self.n_features = n_features
      self.hidden_dim = hidden_dim
      self.num_layers = num_layers
      self.seq_length = seq_length
      self.batch_size = batch_size
      self.minibatch = minibatch
      
      self.layer1 = torch.nn.LSTM(input_size = self.n_features, hidden_size = self.hidden_dim, num_layers = self.num_layers
                                  ,batch_first = True#,dropout = 0.2
                                 )
      self.layer2 = torch.nn.Sequential(torch.nn.Linear(self.hidden_dim,1),torch.nn.LeakyReLU(0.2),torch.nn.Dropout(0.2)) 
      #Adding a minibatch discriminator layer to add a cripple affect to the discriminator so that it needs to generate sequences that are different from each other.
      if self.minibatch > 0:
        self.mb1= MinibatchDiscrimination(self.seq_length,self.minibatch,minibatch_normal_init)
        self.out = torch.nn.Sequential(torch.nn.Linear(self.seq_length+self.minibatch,1),torch.nn.Sigmoid()) # to make sure the output is between 0 and 1
      else:
        self.out = torch.nn.Sequential(torch.nn.Linear(self.seq_length,1),torch.nn.Sigmoid()) # to make sure the output is between 0 and 1
 
  def init_hidden(self):
      # This line creates a new type as the first parameter tensor
      weight = next(self.parameters()).data
      #This line creates a new tensor of the same type as weight initailised to zero
      hidden = (weight.new(self.num_layers, self.batch_size, self.hidden_dim).zero_().cuda(), weight.new(self.num_layers, self.batch_size, self.hidden_dim).zero_().cuda())
      return hidden
    
  def get_mb_params(self):
      try:
        params = self.mb1.get_parameters()
        return params
      except: 
        if self.minibatch == 0:
            print("Minibatch Discrimination was not configured")
        else:
          print("Issue with get_parameter function")
        return torch.ones((1,1))
      
  
  def forward(self,x,hidden):
      
      x,hidden = self.layer1(x.view(self.batch_size,self.seq_length,1),hidden)
      
      x = self.layer2(x.view(self.batch_size,self.seq_length,self.hidden_dim))
      if self.minibatch > 0:
        x = self.mb1(x.squeeze())
      x = self.out(x.squeeze())
      
      return x,hidden

"""##Generator"""

class Generator(torch.nn.Module):
  def __init__(self,seq_length,batch_size,n_features = 1, hidden_dim = 50, num_layers = 2, tanh_output = False):
      super(Generator,self).__init__()
      self.n_features = n_features
      self.hidden_dim = hidden_dim
      self.num_layers = num_layers
      self.seq_length = seq_length
      self.batch_size = batch_size
      self.tanh_output = tanh_output
      

      
      self.layer1 = torch.nn.LSTM(input_size = self.n_features, hidden_size = self.hidden_dim, 
                                  num_layers = self.num_layers,batch_first = True#,dropout = 0.2,
                                 )
      if self.tanh_output == True:
        self.out = torch.nn.Sequential(torch.nn.Linear(self.hidden_dim,1),torch.nn.Tanh()) # to make sure the output is between 0 and 1 - removed ,torch.nn.Sigmoid()
      else:
        self.out = torch.nn.Linear(self.hidden_dim,1) 
      
  def init_hidden(self):
      weight = next(self.parameters()).data
      hidden = (weight.new(self.num_layers, self.batch_size, self.hidden_dim).zero_().cuda(), weight.new(self.num_layers, self.batch_size, self.hidden_dim).zero_().cuda())
      return hidden
  
  def forward(self,x,hidden):
      
      x,hidden = self.layer1(x.view(self.batch_size,self.seq_length,1),hidden)
      
      x = self.out(x)
      
      return x #,hidden

def noise(batch_size, features):
  noise_vec = torch.randn(batch_size, features).cuda()
  return noise_vec

"""#Defining Parameters"""

seq_length = sine_data[0].size()[0] #Number of features

hidden_nodes_g = 50
hidden_nodes_d = 50
minibatch_layer = 0
minibatch_normal_init_ = True
layers = 2
D_rounds = 1
G_rounds = 3

num_epoch = 36
learning_rate = 0.0002
tanh_layer = False

"""####Generator and Discriminator training time"""

minibatch_out = [0,3,5,8,10]
for minibatch_layer in minibatch_out:
  path = "./Run_"+str(minibatch_layer)
  os.mkdir(path)

  dict = {'data' : source_filename, 
        'sample_size' : sample_size, 
        'seq_length' : seq_length,
        'num_layers': layers, 
        'hidden_dims_generator': hidden_nodes_g, 
        'hidden_dims_discriminator':hidden_nodes_d,
        'D_rounds': D_rounds,
        'G_rounds': G_rounds,
        'num_epoch':num_epoch,
        'learning_rate' : learning_rate,
        'tanh_layer': tanh_layer,
        'minibatch_layer': minibatch_layer,
        'minibatch_normal_init_':minibatch_normal_init_}

  json = js.dumps(dict)
  f = open(path+"/settings.json","w")
  f.write(json)
  f.close()
  
  generator = Generator(seq_length,sample_size,hidden_dim = hidden_nodes_g, num_layers = layers, tanh_output = tanh_layer).cuda()
  discriminator = Discriminator(seq_length, sample_size,minibatch_normal_init = minibatch_normal_init_, hidden_dim = hidden_nodes_d, num_layers = layers, minibatch = minibatch_layer).cuda()
  d_optimizer = torch.optim.Adam(discriminator.parameters(),lr = learning_rate)
  g_optimizer = torch.optim.Adam(generator.parameters(),lr = learning_rate)
  #Loss function 
  loss = torch.nn.BCELoss()

  generator.train()
  discriminator.train()

  G_losses = []
  D_losses = []
  mmd_list = []
  series_list = np.zeros((1,seq_length))

  for n in tqdm(range(num_epoch)):
     # for k in range(1):

      for n_batch, sample_data in enumerate(data_loader):

        for d in range(D_rounds):
      ### TRAIN DISCRIMINATOR ON FAKE DATA
          discriminator.zero_grad()

          h_d = discriminator.init_hidden()
          h_g = generator.init_hidden()

          #Generating the noise and label data
          noise_sample = Variable(noise(len(sample_data),seq_length))

          #Use this line if generator outputs hidden states: dis_fake_data, (h_g_n,c_g_n) = generator.forward(noise_sample,h_g)
          dis_fake_data = generator.forward(noise_sample,h_g).detach()

          y_pred_fake, (h_d_n,c_d_n) = discriminator(dis_fake_data,h_d)

          loss_fake = loss(y_pred_fake,torch.zeros([len(sample_data),1]).cuda())
          loss_fake.backward()    

          #Train discriminator on real data   
          h_d = discriminator.init_hidden()
          real_data = Variable(sample_data.float()).cuda()    
          y_pred_real,(h_d_n,c_d_n)= discriminator.forward(real_data,h_d)

          loss_real = loss(y_pred_real,torch.ones([len(sample_data),1]).cuda())
          loss_real.backward()

          d_optimizer.step() #Updating the weights based on the predictions for both real and fake calculations.


        for g in range(G_rounds):
        #Train Generator  
          generator.zero_grad()
          h_g = generator.init_hidden()
          h_d = discriminator.init_hidden()
          noise_sample = Variable(noise(len(sample_data), seq_length))


          #Use this line if generator outputs hidden states: gen_fake_data, (h_g_n,c_g_n) = generator.forward(noise_sample,h_g)
          gen_fake_data = generator.forward(noise_sample,h_g)
          y_pred_gen, (h_d_n,c_d_n)= discriminator(gen_fake_data,h_d)

          error_gen = loss(y_pred_gen,torch.ones([len(sample_data),1]).cuda())
          error_gen.backward()
          g_optimizer.step()





       

        if n_batch == num_batches - 1:
          G_losses.append(error_gen.item())
          D_losses.append((loss_real+loss_fake).item())

          

          torch.save(generator.state_dict(), path+'/generator_state_'+str(n)+'.pt')
          torch.save(discriminator.state_dict(),path+ '/discriminator_state_'+str(n)+'.pt')

          
          # Check how the generator is doing by saving G's output on fixed_noise
        
          with torch.no_grad():
              h_g = generator.init_hidden()
              fake = generator(noise(len(sample_data), seq_length),h_g).detach().cpu()
              generated_sample = torch.zeros(1,seq_length).cuda()
              
              
              for test_batch in range(0,int(len(data_test)/sample_size)):
                noise_sample_test = noise(sample_size, seq_length)
                h_g = generator.init_hidden()
                generated_data = generator.forward(noise_sample_test,h_g).detach().squeeze()
                generated_sample = torch.cat((generated_sample,generated_data),dim = 0)


              # Getting the MMD Statistic for each Training Epoch
              generated_sample = generated_sample[1:][:]
              sigma = [pairwisedistances(data_test[:].type(torch.DoubleTensor),generated_sample.type(torch.DoubleTensor).squeeze())] 
              mmd = MMDStatistic(len(data_test[:]),generated_sample.size(0))
              mmd_eval = mmd(data_test[:].type(torch.DoubleTensor),generated_sample.type(torch.DoubleTensor).squeeze(),sigma, ret_matrix=False)
              mmd_list.append(mmd_eval.item())



              series_list = np.append(series_list,fake[0].numpy().reshape((1,seq_length)),axis=0)
 
  
  
  
  #Dumping the errors and mmd evaluations for each training epoch.
  with open(path+'/generator_losses.txt', 'wb') as fp:
      pickle.dump(G_losses, fp)
  with open(path+'/discriminator_losses.txt', 'wb') as fp:
      pickle.dump(D_losses, fp)   
  with open(path+'/mmd_list.txt', 'wb') as fp:
      pickle.dump(mmd_list, fp)
  
  #Plotting the error graph
  plt.plot(G_losses,'-r',label='Generator Error')
  plt.plot(D_losses, '-b', label = 'Discriminator Error')
  plt.title('GAN Errors in Training')
  plt.legend()
  plt.savefig(path+'/GAN_errors.png')
  plt.close()
  
  
  #Plot a figure for each training epoch with the MMD value in the title
  i = 0
  while i < num_epoch:
    if i%3==0:
      fig, ax = plt.subplots(3,1,constrained_layout=True)
      fig.suptitle("Generated fake data")
    for j in range(0,3):
      ax[j].plot(series_list[i][:])
      ax[j].set_title('Epoch '+str(i)+ ', MMD: %.4f' % (mmd_list[i]))
      i = i+1
    plt.savefig(path+'/Training_Epoch_Samples_MMD_'+str(i)+'.png')
    plt.close(fig)
    

  #Checking the diversity of the samples:
  generator.eval()
  h_g = generator.init_hidden()
  test_noise_sample = noise(sample_size, seq_length)
  gen_data= generator.forward(test_noise_sample,h_g).detach()


  plt.title("Generated Sine Waves")
  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-b')
  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-r')
  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-g')
  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-', color = 'orange')
  plt.savefig(path+'/Generated_Data_Sample1.png')
  plt.close()